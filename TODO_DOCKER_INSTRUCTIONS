## Running nino with Docker

You can containerize nino using Docker to simplify deployment and ensure consistent environments across different systems. This section provides instructions on how to build and run nino using Docker.

### Prerequisites

-   **Docker Installed**: Ensure that [Docker](https://docs.docker.com/get-docker/) is installed on your system.

### Building the Docker Image

1. **Create a Dockerfile**

    In the root directory of the nino project, create a file named `Dockerfile` with the following content:

    ```dockerfile
    # Build Stage
    FROM golang:1.23 AS builder

    # Set the working directory inside the container
    WORKDIR /app

    # Copy the Go module files and download dependencies
    COPY go.mod go.sum ./
    RUN go mod download

    # Copy the rest of the application code
    COPY . .

    # Build the Go application
    RUN make build

    # Run Stage
    FROM debian:buster-slim

    # Set the working directory in the new image
    WORKDIR /app

    # Copy the built binary from the builder stage
    COPY --from=builder /app/nino /app/nino

    # Expose any necessary ports (if applicable)
    # EXPOSE 8080

    # Set the entrypoint command
    ENTRYPOINT ["./nino"]
    ```

    This Dockerfile uses a multi-stage build to keep the final image small.

2. **Build the Docker Image**

    Open a terminal in the root directory of the project and run:

    ```bash
    docker build -t nino:latest .
    ```

    This command builds the Docker image and tags it as `nino:latest`.

### Running nino Container

Since nino depends on the Ollama server, you need to ensure that the container running nino can communicate with the Ollama server.

#### Option 1: Connect to an Ollama Server Running on the Host Machine

If you have Ollama running on your host machine, you can run the nino Docker container and connect to the Ollama server running on your host.

1. **Run nino Docker Container**

    ```bash
    docker run --rm -it \
      --add-host=host.docker.internal:host-gateway \
      nino:latest \
      -model llama3.1 \
      -prompt "What is the capital of France?" \
      -url http://host.docker.internal:11434/api/generate
    ```

    - The `--add-host` flag maps `host.docker.internal` to the host gateway, allowing the container to communicate with services running on the host machine.
    - Replace the prompt and model as needed.

#### Option 2: Run Ollama Server Inside Another Docker Container

You can run both Ollama and nino in Docker containers and connect them using a Docker network.

1. **Create a Docker Network**

    ```bash
    docker network create nino-net
    ```

2. **Create a Docker Image for Ollama**

    Create a `Dockerfile` for Ollama (if one doesn't exist) and build the image. For example:

    ```dockerfile
    # Ollama Dockerfile
    FROM ubuntu:20.04

    # Install necessary dependencies
    RUN apt-get update && apt-get install -y curl

    # Install Ollama (replace with actual installation steps)
    RUN curl -L https://example.com/install-ollama.sh | bash

    # Expose Ollama's default port
    EXPOSE 11434

    # Start the Ollama server
    CMD ["ollama", "serve"]
    ```

    Build the Ollama image:

    ```bash
    docker build -t ollama:latest -f OllamaDockerfile .
    ```

3. **Run Ollama Docker Container**

    ```bash
    docker run -d \
      --name ollama-server \
      --network nino-net \
      ollama:latest
    ```

4. **Run nino Docker Container**

    ```bash
    docker run --rm -it \
      --network nino-net \
      nino:latest \
      -model llama3.1 \
      -prompt "What is the capital of France?" \
      -url http://ollama-server:11434/api/generate
    ```

    - The `--network nino-net` flag connects the container to the `nino-net` network.
    - The `-url http://ollama-server:11434/api/generate` argument tells nino to connect to the Ollama server running in the `ollama-server` container.

### Using Docker Compose

To simplify running multiple containers, you can use Docker Compose.

1. **Create a `docker-compose.yml` File**

    In the root directory of the project, create a `docker-compose.yml` file with the following content:

    ```yaml
    version: "3.8"
    services:
        ollama-server:
            image: ollama:latest
            networks:
                - nino-net
            ports:
                - "11434:11434"
            command: ollama serve

        nino:
            image: nino:latest
            depends_on:
                - ollama-server
            networks:
                - nino-net
            entrypoint: ["./nino"]
            command: ["-model", "llama3.1", "-prompt", "What is the capital of France?", "-url", "http://ollama-server:11434/api/generate"]

    networks:
        nino-net:
            driver: bridge
    ```

    - Replace the prompt and model as needed.
    - Ensure that the `ollama:latest` and `nino:latest` images are built as per previous steps.

2. **Run Docker Compose**

    In the terminal, run:

    ```bash
    docker-compose up
    ```

    This command starts both the Ollama server and nino services, with nino sending prompts to the Ollama server.

### Notes and Considerations

-   **Docker Image for Ollama**: If there is an official Docker image for Ollama, use that image instead of building your own. Update the `docker-compose.yml` and Docker commands accordingly.
-   **Networking**: When using Docker, containers communicate over networks. Ensure that both nino and Ollama are on the same Docker network.
-   **Environment Variables**: You can pass environment variables to the Docker container using the `-e` flag in `docker run` or the `environment` section in `docker-compose.yml`.
-   **Persistent Storage**: If you need to persist data or share files between your host and the container, consider using Docker volumes or bind mounts.
-   **Platform Compatibility**: The `--network="host"` flag works differently on Linux, macOS, and Windows. On macOS and Windows, you might need to use `host.docker.internal` to reference the host machine.

### Example Docker Run Command

Here is an example of running nino in Docker, connecting to an Ollama server running on the host:

```bash
docker run --rm -it \
  --add-host=host.docker.internal:host-gateway \
  nino:latest \
  -model llama3.1 \
  -prompt "Who wrote 'To be, or not to be'?" \
  -url http://host.docker.internal:11434/api/generate
```

### Cleaning Up

To stop and remove containers and networks created by Docker Compose:

```bash
docker-compose down
```

## Conclusion

Running nino with Docker allows for a consistent and isolated environment, making it easier to manage dependencies and deployments. Whether you choose to connect to an Ollama server running on your host machine or containerize both nino and Ollama, Docker provides the flexibility to suit your development and deployment needs.
