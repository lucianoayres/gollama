# Gollama CLI Tool

![gollama-banner-v1](https://github.com/user-attachments/assets/6dec07fd-2adb-4eb8-8e67-5b2d9445c209)

## About Gollama

Gollama is a Go-based command-line tool designed to interact with local language models via the [Ollama](https://github.com/jmorganca/ollama) command-line interface (CLI). It allows users to send prompts to models, receive streaming responses in real-time, and specify model configurations through simple command-line arguments.

### Motivation

Gollama was created to provide a seamless way to interact with local language models via Ollama's serve mode. While Ollama enables model interaction, Gollama enhances this by displaying the entire model response in the terminal and allowing users to save the output to a file.

## Demo

![btcq_demo](https://github.com/user-attachments/assets/180b8196-d1d2-44b9-95f3-19deeed4d808)

## Features

-   **Streaming Responses**: Gollama handles and prints streaming responses generated by the Ollama CLI as the model processes the input.
-   **Command Execution**: Gollama leverages the Ollama CLI to interact with language models by invoking commands and processing the output, making it a seamless extension for local model interaction.

## Ollama Dependency

Gollama relies on the [Ollama CLI tool](https://github.com/jmorganca/ollama) to interact with local language models. Ollama must be installed and running on your machine for Gollama to function properly.

### Install Ollama

Follow the instructions on the [Ollama GitHub repository](https://github.com/jmorganca/ollama) to install and set up Ollama. Ensure that Ollama is available in your systemâ€™s `$PATH`.

### Start the Ollama Server

Once Ollama is installed, you need to start the Ollama server on `localhost:11434` for Gollama to communicate with it:

1. **Start the Ollama Server**: This command will run the Ollama server on `localhost:11434` (default port).

    ```bash
    ollama serve
    ```

2. **Run the Model**: To run the desired model (e.g., `llama3.1`), execute the following command in a separate terminal window:
    ```bash
    ollama run llama3.1
    ```

> **Note:** The `-model` parameter in Gollama **must match** the model that you run on Ollama. For example, if you start `llama3.1` in Ollama, you must pass `llama3.1` as the `-model` in Gollama. Otherwise, Gollama will not be able to communicate with the correct model.

Ollama should now be running, and Gollama can interact with it by sending prompts.

## Requirements

-   Go 1.16+ installed on your system
-   [Ollama](https://github.com/jmorganca/ollama) installed and running on `localhost:11434`
-   Ensure that the Ollama server is running via `ollama serve`

## Installation

1. Clone this repository:

    ```bash
    git clone https://github.com/lucianoayres/gollama.git
    cd gollama
    ```

2. Build the project:

    ```bash
    make build
    ```

## Usage

After building the project and ensuring that the Ollama server is running, you can run Gollama with the following commands:

```bash
./gollama -model "llama3.1" -prompt "Explain LLMs like I'm five"
```

### Command-line Flags:

-   `-model` : The model to use (default: "llama3.1"). **This must match the model that is currently running on Ollama.**
-   `-prompt` : The prompt to send to the language model (required)

If no prompt is provided, the tool will exit with an error and provide usage instructions.

### Example

```bash
./gollama -model llama3.1 -prompt "What is the capital of France?"
```

### Output

Gollama will print the model's response to the console as it receives streaming output by invoking the Ollama CLI.

#### Saving the Output to a File

You can optionally save the model's output to a file with the following command:

```bash
./gollama -model llama3.1 -prompt "What is the capital of France?" | tee answer.txt
```

## Troubleshooting

The application includes basic error handling for:

-   Connection problems with the model
-   Problems processing the output of the Ollama command

In the case of an error, the tool will display a relevant error message and exit.

### Ensure Ollama Server is Running

If Gollama encounters connection issues, ensure that the Ollama server is running on the correct port (`localhost:11434`), as Gollama depends on this connection to interact with the model.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

## Contribution

Contributions are welcome! Please fork the repository and submit a pull request if you'd like to propose any changes.
